/**
 * Receptionist AI Chat API Route
 * Streaming chat endpoint for Emma (the virtual receptionist widget)
 */

import { streamText } from 'ai';
import { openai } from '@/lib/ai/providers';
import { AI_CONFIG } from '@/lib/ai/config';
import { buildAgentSystemPrompt } from '@/lib/ai/personas';

export const runtime = 'edge';

interface MessagePart {
  type: 'text';
  text?: string;
}

interface IncomingMessage {
  role: 'user' | 'assistant' | 'system';
  content?: string;
  parts?: MessagePart[];
}

export async function POST(req: Request) {
  try {
    const { messages } = await req.json();

    // Extract text content from messages (handles both old and new formats)
    const getMessageContent = (msg: IncomingMessage): string => {
      if (msg.parts && msg.parts.length > 0) {
        return msg.parts
          .filter((part): part is MessagePart & { text: string } => part.type === 'text' && !!part.text)
          .map(part => part.text)
          .join('');
      }
      return msg.content || '';
    };

    // Format messages for the AI SDK (text only â€” no image support for receptionist)
    const formattedMessages = messages
      .filter((msg: IncomingMessage) => msg.role === 'user' || msg.role === 'assistant')
      .map((msg: IncomingMessage) => ({
        role: msg.role as 'user' | 'assistant',
        content: getMessageContent(msg),
      }));

    const result = streamText({
      model: openai(AI_CONFIG.openai.chat),
      system: buildAgentSystemPrompt('receptionist'),
      messages: formattedMessages,
      maxOutputTokens: 512,
      temperature: 0.7,
    });

    return result.toUIMessageStreamResponse();
  } catch (error) {
    console.error('Receptionist API error:', error);
    return new Response(
      JSON.stringify({ error: 'Failed to process request' }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
}
